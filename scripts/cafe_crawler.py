#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ì¹´í˜ ì „ì²´ ê¸€ í¬ë¡¤ëŸ¬
IRIS ê´€ë ¨ ì •ë³´ ë° ë…¸ë£¨íŒ… ê´€ë ¨ ê¸€ ìˆ˜ì§‘
"""

import os
import asyncio
import json
from datetime import datetime
from dotenv import load_dotenv
from playwright.async_api import async_playwright

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv('config/local.env')

async def crawl_cafe_articles():
    """ì¹´í˜ ì „ì²´ ê¸€ í¬ë¡¤ë§"""

    naver_id = os.getenv('NAVER_ID')
    naver_pw = os.getenv('NAVER_PW')
    cafe_url = os.getenv('NAVER_CAFE_URL')

    collected_data = {
        'crawl_time': datetime.now().isoformat(),
        'iris_articles': [],
        'noruting_articles': [],
        'other_articles': [],
        'categories': {}
    }

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()
        page = await context.new_page()

        try:
            # ë¡œê·¸ì¸
            print("ğŸ”„ ë„¤ì´ë²„ ë¡œê·¸ì¸...")
            await page.goto("https://nid.naver.com/nidlogin.login")
            await page.fill('input[name="id"]', naver_id)
            await page.fill('input[name="pw"]', naver_pw)
            await page.click('button[type="submit"]')
            await page.wait_for_timeout(5000)

            # ëª¨ë°”ì¼ ì¸ì¦ ëŒ€ê¸°
            if "nidlogin.login" in page.url:
                print("ğŸ“± ëª¨ë°”ì¼ ì¸ì¦ ëŒ€ê¸°...")
                for i in range(24):
                    await page.wait_for_timeout(5000)
                    if "nidlogin.login" not in page.url:
                        break
                    print(f"â³ ì¸ì¦ ëŒ€ê¸° ì¤‘... ({(i+1)*5}/120ì´ˆ)")

            # ì¹´í˜ ë©”ì¸ìœ¼ë¡œ ì´ë™
            print("ğŸ”„ ì¹´í˜ ë©”ì¸ìœ¼ë¡œ ì´ë™...")
            await page.goto(cafe_url)
            await page.wait_for_timeout(3000)

            # ì¹´í…Œê³ ë¦¬ ì •ë³´ ìˆ˜ì§‘
            try:
                menu_elements = await page.query_selector_all('.menu-item a')
                for element in menu_elements:
                    menu_text = await element.text_content()
                    menu_url = await element.get_attribute('href')
                    if menu_text and menu_url:
                        collected_data['categories'][menu_text] = menu_url
                        print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {menu_text}")
            except Exception as e:
                print(f"ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")

            # ê²Œì‹œíŒ ëª©ë¡ í™•ì¸
            boards = ['L', 'C']  # ì¼ë°˜ê²Œì‹œíŒ, ê³µì§€ì‚¬í•­ ë“±

            for board_type in boards:
                print(f"ğŸ“‹ {board_type} ê²Œì‹œíŒ ê¸€ ëª©ë¡ ìˆ˜ì§‘...")

                # ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§
                for page_num in range(1, 6):  # 1~5í˜ì´ì§€
                    board_url = f"{cafe_url}?boardtype={board_type}&page={page_num}"

                    try:
                        await page.goto(board_url)
                        await page.wait_for_timeout(2000)

                        # ê¸€ ëª©ë¡ ì¶”ì¶œ
                        articles = await page.query_selector_all('.article-board .article')

                        for article in articles:
                            try:
                                # ê¸€ ì œëª©
                                title_elem = await article.query_selector('.article-title a')
                                if not title_elem:
                                    continue

                                title = await title_elem.text_content()
                                article_url = await title_elem.get_attribute('href')

                                # ì‘ì„±ì
                                author_elem = await article.query_selector('.article-nickname')
                                author = await author_elem.text_content() if author_elem else "Unknown"

                                # ì‘ì„±ì¼
                                date_elem = await article.query_selector('.article-date')
                                date = await date_elem.text_content() if date_elem else "Unknown"

                                # ì¡°íšŒìˆ˜
                                view_elem = await article.query_selector('.article-view')
                                views = await view_elem.text_content() if view_elem else "0"

                                article_data = {
                                    'title': title.strip(),
                                    'url': article_url,
                                    'author': author.strip(),
                                    'date': date.strip(),
                                    'views': views.strip(),
                                    'board_type': board_type
                                }

                                # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
                                title_lower = title.lower()
                                if any(keyword in title_lower for keyword in ['iris', 'ì•„ì´ë¦¬ìŠ¤', 'ir', 'ì„¤ì¹˜', 'ì„¤ì •', 'í™˜ê²½']):
                                    article_data['category'] = 'IRIS'
                                    collected_data['iris_articles'].append(article_data)
                                    print(f"ğŸ¯ IRIS ê´€ë ¨ ê¸€: {title[:50]}...")
                                elif any(keyword in title_lower for keyword in ['ë…¸ë£¨íŒ…', 'noruting', 'ë£¨íŒ…', 'root']):
                                    article_data['category'] = 'ë…¸ë£¨íŒ…'
                                    collected_data['noruting_articles'].append(article_data)
                                    print(f"ğŸ”’ ë…¸ë£¨íŒ… ê´€ë ¨ ê¸€: {title[:50]}...")
                                else:
                                    article_data['category'] = 'ê¸°íƒ€'
                                    collected_data['other_articles'].append(article_data)

                            except Exception as e:
                                continue

                        print(f"âœ… {board_type} ê²Œì‹œíŒ {page_num}í˜ì´ì§€ ì™„ë£Œ")

                    except Exception as e:
                        print(f"{board_type} ê²Œì‹œíŒ {page_num}í˜ì´ì§€ ì˜¤ë¥˜: {e}")
                        continue

            # IRIS ê´€ë ¨ ê¸€ ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘
            print("ğŸ¯ IRIS ê´€ë ¨ ê¸€ ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘...")
            for i, article in enumerate(collected_data['iris_articles'][:10]):  # ìƒìœ„ 10ê°œë§Œ ìƒì„¸ ìˆ˜ì§‘
                try:
                    print(f"ğŸ“– ê¸€ ì½ëŠ” ì¤‘: {article['title'][:30]}...")
                    await page.goto(article['url'])
                    await page.wait_for_timeout(2000)

                    # ê¸€ ë‚´ìš© ì¶”ì¶œ
                    content_elem = await page.query_selector('.se-main-container')
                    if content_elem:
                        content = await content_elem.text_content()
                        collected_data['iris_articles'][i]['content'] = content[:1000] + "..." if len(content) > 1000 else content

                        # ëŒ“ê¸€ë„ ìˆ˜ì§‘
                        comments = []
                        comment_elements = await page.query_selector_all('.comment-list .comment-item')
                        for comment_elem in comment_elements[:5]:  # ìƒìœ„ 5ê°œ ëŒ“ê¸€ë§Œ
                            try:
                                comment_text = await comment_elem.text_content()
                                if comment_text:
                                    comments.append(comment_text.strip())
                            except:
                                continue
                        collected_data['iris_articles'][i]['comments'] = comments

                    await page.wait_for_timeout(1000)

                except Exception as e:
                    print(f"ê¸€ ë‚´ìš© ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                    continue

            # ë…¸ë£¨íŒ… ê´€ë ¨ ê¸€ ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘
            print("ğŸ”’ ë…¸ë£¨íŒ… ê´€ë ¨ ê¸€ ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘...")
            for i, article in enumerate(collected_data['noruting_articles']):
                try:
                    print(f"ğŸ“– ê¸€ ì½ëŠ” ì¤‘: {article['title'][:30]}...")
                    await page.goto(article['url'])
                    await page.wait_for_timeout(2000)

                    content_elem = await page.query_selector('.se-main-container')
                    if content_elem:
                        content = await content_elem.text_content()
                        collected_data['noruting_articles'][i]['content'] = content[:1000] + "..." if len(content) > 1000 else content

                except Exception as e:
                    print(f"ê¸€ ë‚´ìš© ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                    continue

            # ë°ì´í„° ì €ì¥
            with open('cafe_data.json', 'w', encoding='utf-8') as f:
                json.dump(collected_data, f, ensure_ascii=False, indent=2)

            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“Š IRIS ê´€ë ¨ ê¸€: {len(collected_data['iris_articles'])}ê°œ")
            print(f"ğŸ”’ ë…¸ë£¨íŒ… ê´€ë ¨ ê¸€: {len(collected_data['noruting_articles'])}ê°œ")
            print(f"ğŸ“„ ê¸°íƒ€ ê¸€: {len(collected_data['other_articles'])}ê°œ")

            return collected_data

        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return None

        finally:
            await browser.close()

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ë„¤ì´ë²„ ì¹´í˜ ì „ì²´ ê¸€ í¬ë¡¤ë§ ì‹œì‘")
    data = await crawl_cafe_articles()

    if data:
        print("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    else:
        print("âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")

if __name__ == "__main__":
    asyncio.run(main())