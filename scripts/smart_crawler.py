#!/usr/bin/env python3
"""
ìŠ¤ë§ˆíŠ¸ ì¹´í˜ í¬ë¡¤ëŸ¬
JavaScript ë Œë”ë§ í˜ì´ì§€ì™€ ë„¤ì´ë²„ ì¹´í˜ ìµœì‹  êµ¬ì¡°ì— ìµœì í™”
"""

import os
import asyncio
import json
from datetime import datetime
from dotenv import load_dotenv
from playwright.async_api import async_playwright

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv('config/local.env')

async def smart_crawl_cafe():
    """ìŠ¤ë§ˆíŠ¸ ì¹´í˜ í¬ë¡¤ë§"""

    naver_id = os.getenv('NAVER_ID')
    naver_pw = os.getenv('NAVER_PW')
    cafe_url = os.getenv('NAVER_CAFE_URL')

    collected_data = {
        'crawl_time': datetime.now().isoformat(),
        'iris_articles': [],
        'noruting_articles': [],
        'installation_guides': [],
        'bot_tutorials': [],
        'technical_posts': []
    }

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()
        page = await context.new_page()

        try:
            # ë¡œê·¸ì¸
            print("ğŸ”„ ë„¤ì´ë²„ ë¡œê·¸ì¸...")
            await page.goto("https://nid.naver.com/nidlogin.login")
            await page.fill('input[name="id"]', naver_id)
            await page.fill('input[name="pw"]', naver_pw)
            await page.click('button[type="submit"]')
            await page.wait_for_timeout(5000)

            # ëª¨ë°”ì¼ ì¸ì¦ ëŒ€ê¸°
            if "nidlogin.login" in page.url:
                print("ğŸ“± ëª¨ë°”ì¼ ì¸ì¦ ëŒ€ê¸°...")
                for i in range(24):
                    await page.wait_for_timeout(5000)
                    if "nidlogin.login" not in page.url:
                        break

            # ì¹´í˜ ë©”ì¸ìœ¼ë¡œ ì´ë™ ë° ëŒ€ê¸°
            print("ğŸ”„ ì¹´í˜ ë©”ì¸ìœ¼ë¡œ ì´ë™...")
            await page.goto(cafe_url)
            await page.wait_for_timeout(5000)  # JS ë Œë”ë§ ëŒ€ê¸°

            # ê²€ìƒ‰ ê¸°ëŠ¥ í™œìš©
            search_keywords = ['IRIS', 'iris', 'ì•„ì´ë¦¬ìŠ¤', 'ì„¤ì¹˜', 'ì„¤ì •', 'ë´‡', 'ìë™ì‘ë‹µ', 'ë…¸ë£¨íŒ…', 'ë£¨íŒ…']

            for keyword in search_keywords:
                print(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰: {keyword}")

                try:
                    # ê²€ìƒ‰ì°½ ì°¾ê¸°
                    search_selectors = [
                        'input[placeholder*="ê²€ìƒ‰"]',
                        'input[type="search"]',
                        '.search-input',
                        '#search-input',
                        'input[name="query"]',
                        'input[name="q"]'
                    ]

                    search_input = None
                    for selector in search_selectors:
                        try:
                            search_input = await page.query_selector(selector)
                            if search_input:
                                print(f"âœ… ê²€ìƒ‰ì°½ ë°œê²¬: {selector}")
                                break
                        except:
                            continue

                    if search_input:
                        await search_input.fill(keyword)
                        await page.keyboard.press('Enter')
                        await page.wait_for_timeout(3000)

                        # ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„
                        await analyze_search_results(page, collected_data, keyword)

                        # ë’¤ë¡œê°€ê¸°
                        await page.go_back()
                        await page.wait_for_timeout(2000)

                except Exception as e:
                    print(f"ê²€ìƒ‰ ì˜¤ë¥˜ ({keyword}): {e}")
                    continue

            # ì§ì ‘ URL ë°©ì‹ìœ¼ë¡œ ê²Œì‹œíŒ ì ‘ê·¼ ì‹œë„
            board_urls = [
                f"{cafe_url}?boardtype=L",
                f"{cafe_url}?search.sortType=createdAt",
                f"{cafe_url}?search.query=IRIS"
            ]

            for board_url in board_urls:
                try:
                    print(f"ğŸ”„ ê²Œì‹œíŒ ì§ì ‘ ì ‘ì†: {board_url}")
                    await page.goto(board_url)
                    await page.wait_for_timeout(5000)

                    # í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ
                    content = await page.content()
                    import re

                    # ë„¤ì´ë²„ ì¹´í˜ ê²Œì‹œê¸€ URL íŒ¨í„´
                    article_pattern = r'https://cafe\.naver\.com/f-e/[^\s"]+/articles/\d+'
                    articles = re.findall(article_pattern, content)

                    print(f"ğŸ“„ ë°œê²¬ëœ ê²Œì‹œê¸€: {len(articles)}ê°œ")

                    # ì¤‘ë³µ ì œê±°
                    unique_articles = list(set(articles))
                    print(f"ğŸ“„ ê³ ìœ  ê²Œì‹œê¸€: {len(unique_articles)}ê°œ")

                    # ìƒìœ„ ê²Œì‹œê¸€ë§Œ ë°©ë¬¸
                    for article_url in unique_articles[:10]:
                        try:
                            print(f"ğŸ“– ê¸€ ì½ê¸°: {article_url}")
                            await page.goto(article_url)
                            await page.wait_for_timeout(3000)

                            # ê¸€ ì •ë³´ ì¶”ì¶œ
                            article_info = await extract_article_info(page)
                            if article_info:
                                categorize_article(collected_data, article_info, keyword)

                            await page.wait_for_timeout(1000)

                        except Exception as e:
                            print(f"ê¸€ ì½ê¸° ì˜¤ë¥˜: {e}")
                            continue

                    await page.wait_for_timeout(2000)

                except Exception as e:
                    print(f"ê²Œì‹œíŒ ì ‘ì† ì˜¤ë¥˜: {e}")
                    continue

            # ê²°ê³¼ ì €ì¥
            with open('smart_cafe_data.json', 'w', encoding='utf-8') as f:
                json.dump(collected_data, f, ensure_ascii=False, indent=2)

            print_results(collected_data)
            return collected_data

        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            await page.screenshot(path="smart_crawler_error.png")
            return None

        finally:
            await browser.close()

async def analyze_search_results(page, collected_data, keyword):
    """ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„"""
    try:
        # ë‹¤ì–‘í•œ ê²Œì‹œê¸€ ì„ íƒì ì‹œë„
        article_selectors = [
            '.article-item',
            '.list-item',
            '[data-article-id]',
            '.cafe-article',
            '.search-result-item',
            'a[href*="articles/"]'
        ]

        for selector in article_selectors:
            try:
                elements = await page.query_selector_all(selector)
                if elements:
                    print(f"âœ… ê²€ìƒ‰ ê²°ê³¼ ë°œê²¬ ({selector}): {len(elements)}ê°œ")

                    for element in elements[:5]:  # ìƒìœ„ 5ê°œë§Œ
                        try:
                            # ë§í¬ ì¶”ì¶œ
                            if element.tag_name == 'a':
                                article_url = await element.get_attribute('href')
                            else:
                                link_elem = await element.query_selector('a[href*="articles/"]')
                                if link_elem:
                                    article_url = await link_elem.get_attribute('href')
                                else:
                                    continue

                            if article_url and 'articles/' in article_url:
                                # ì œëª© ì¶”ì¶œ
                                title = await element.text_content()
                                print(f"  ğŸ“ {title[:50]}...")

                                # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ì„ ìœ„í•´ ë‚˜ì¤‘ì— ë°©ë¬¸í•  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                                collected_data['iris_articles'].append({
                                    'title': title.strip(),
                                    'url': article_url,
                                    'keyword': keyword
                                })

                        except Exception as e:
                            continue
                    break
            except:
                continue

    except Exception as e:
        print(f"ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ì˜¤ë¥˜: {e}")

async def extract_article_info(page):
    """ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ"""
    try:
        # ì œëª©
        title_selectors = [
            '.title_area',
            '.article-title',
            '.board-title',
            'h1', 'h2', 'h3',
            '.tit_area'
        ]

        title = "ì œëª© ì—†ìŒ"
        for selector in title_selectors:
            try:
                elem = await page.query_selector(selector)
                if elem:
                    title = await elem.text_content()
                    title = title.strip()
                    if title and len(title) > 3:
                        break
            except:
                continue

        # ë‚´ìš©
        content_selectors = [
            '.se-main-container',
            '.article-content',
            '.board-content',
            '.post-content',
            '#postContent'
        ]

        content = "ë‚´ìš© ì—†ìŒ"
        for selector in content_selectors:
            try:
                elem = await page.query_selector(selector)
                if elem:
                    content = await elem.text_content()
                    content = content.strip()
                    if content and len(content) > 10:
                        content = content[:1000] + "..." if len(content) > 1000 else content
                        break
            except:
                continue

        # ì‘ì„±ì
        author_selectors = [
            '.nickname',
            '.author',
            '.writer',
            '.user-id'
        ]

        author = "ì‘ì„±ì ì—†ìŒ"
        for selector in author_selectors:
            try:
                elem = await page.query_selector(selector)
                if elem:
                    author = await elem.text_content()
                    author = author.strip()
                    if author:
                        break
            except:
                continue

        return {
            'title': title,
            'content': content,
            'author': author,
            'url': page.url
        }

    except Exception as e:
        print(f"ê¸€ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None

def categorize_article(collected_data, article_info, keyword):
    """ê²Œì‹œê¸€ ë¶„ë¥˜"""
    title = article_info['title'].lower()
    content = article_info['content'].lower()

    # IRIS ê´€ë ¨
    if any(kw in title + content for kw in ['iris', 'ì•„ì´ë¦¬ìŠ¤', 'ì„¤ì¹˜', 'ì„¤ì •', 'í™˜ê²½']):
        if 'ì„¤ì¹˜' in title + content:
            collected_data['installation_guides'].append(article_info)
        else:
            collected_data['iris_articles'].append(article_info)

    # ë…¸ë£¨íŒ… ê´€ë ¨
    elif any(kw in title + content for kw in ['ë…¸ë£¨íŒ…', 'noruting', 'ë£¨íŒ…', 'root']):
        collected_data['noruting_articles'].append(article_info)

    # ë´‡ íŠœí† ë¦¬ì–¼
    elif any(kw in title + content for kw in ['ë´‡', 'ìë™ì‘ë‹µ', 'ì±—ë´‡', 'ë©”ì‹ ì €ë´‡']):
        collected_data['bot_tutorials'].append(article_info)

    # ê¸°ìˆ  í¬ìŠ¤íŠ¸
    elif any(kw in title + content for kw in ['ê°œë°œ', 'ì½”ë”©', 'í”„ë¡œê·¸ë˜ë°', 'api', 'ì†ŒìŠ¤']):
        collected_data['technical_posts'].append(article_info)

def print_results(collected_data):
    """ê²°ê³¼ ì¶œë ¥"""
    print("\n" + "="*50)
    print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
    print("="*50)
    print(f"ğŸ¯ IRIS ê´€ë ¨ ê¸€: {len(collected_data['iris_articles'])}ê°œ")
    print(f"ğŸ”§ ì„¤ì¹˜ ê°€ì´ë“œ: {len(collected_data['installation_guides'])}ê°œ")
    print(f"ğŸ¤– ë´‡ íŠœí† ë¦¬ì–¼: {len(collected_data['bot_tutorials'])}ê°œ")
    print(f"ğŸ”’ ë…¸ë£¨íŒ… ê´€ë ¨: {len(collected_data['noruting_articles'])}ê°œ")
    print(f"ğŸ’» ê¸°ìˆ  í¬ìŠ¤íŠ¸: {len(collected_data['technical_posts'])}ê°œ")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ìŠ¤ë§ˆíŠ¸ ì¹´í˜ í¬ë¡¤ë§ ì‹œì‘")
    data = await smart_crawl_cafe()

    if data:
        print("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    else:
        print("âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")

if __name__ == "__main__":
    asyncio.run(main())